# 2025软工个人编程任务

作业仓库链接（`https://github.com/KKSHLSP/102301319`）  
https://www.learnerhub.net/resources/5049


## 一、PSP表格

# PSP 表格（填写预估与实际用时，单位：小时）

| PSP 阶段                      | 预估 | 实际 | 备注 |
| ----------------------------- | ---- | ---- | ---- |
| 需求分析（包括学习新技术）    | 0.6  | 0.6  | 阅读题目、明确抓取指标 |
| 任务分解与估计                | 0.3  | 0.3  | 拆分抓取/统计/可视化/博客 |
| 设计（总体/详细）             | 0.4  | 0.4  | 设计并发限流、缓存、CLI |
| 编码与实现                    | 2.0  | 2.2  | 爬虫健壮性、管道命令、解析兜底 |
| 代码审查/走查                 | 0.3  | 0.3  | 自查接口与异常处理 |
| 测试（单元/集成）             | 0.6  | 0.6  | pytest + 真实抓取验收 |
| 性能分析与优化                | 0.4  | 0.4  | 并发/延迟权衡、缓存复用 |
| 文档编写（README/博客/注释） | 0.8  | 0.8  | README、博客草稿、使用说明 |
| 复盘与改进                    | 0.3  | 0.3  | 总结风控绕过思路 |
| **合计**                      | 5.7  | 5.9  | 可根据个人实际再微调 |


## 二、任务要求的实现

### 3.1 项目设计与技术栈
- 环节拆分：搜索与抓取 → 本地缓存 → 统计分析（Top 弹幕、关键词分布）→ 词云可视化 → 博客整理。
- 技术栈：`httpx` + `asyncio` 并发抓取、伪装 Cookie 与限流；`pandas` 统计；`openpyxl` 导出；`wordcloud` + `jieba` 词云；`typer` CLI；`pytest` 单测。
- 目录结构：`crawler.py`（抓取）、`analysis.py`（统计）、`visualization.py`（词云）、`persistence.py`（缓存）、`cli.py`（命令）、`tests/`（示例与回归）。

### 3.2 爬虫与数据处理
- 业务流程：调用搜索接口获取 BV → view 接口拿 cid → 弹幕 XML(`dm/list.so`) → 解析 `p` 属性生成纯文本弹幕 → 以 `bvid.json` 缓存到 `data/raw/`。
- 健壮性改造：默认启用缓存；无 Cookie 自动生成轻量 buvid 指纹，降低 412；并发信号量+可调 sleep；搜索结果去重；解析端对 pool/weight 做安全整型转换。
- 数据处理：`build_dataframe` 扁平化；`compute_top_contents` 统计 TopN 弹幕，优先计数、再按最早时间排序；`compute_keyword_distribution` 汇总关键词。

### 3.3 数据统计接口的性能改进
- 并发限流（默认 2，可命令行调至 1）+ `sleep_interval`（默认 0.4，可调）降低 412，同时保持一定吞吐。
- 搜索结果去重，避免重复请求；开启本地 JSON 缓存，重复运行直接命中文件而不再请求接口。
- 解析阶段容错（int 失败即置 None），避免单条脏数据中断整体。
- 性能瓶颈主要在网络 I/O，CPU 侧统计/词云耗时很短（本地 63 视频、4427 弹幕统计 <1s）。

### 3.4 数据结论的可靠性
- 搜索关键词：`大语言模型`、`大模型`、`LLM`，综合排序前若干页，共抓取到 **63 条视频**、**4427 条弹幕**（无 Cookie 情况下部分弹幕接口 412，被记录为空集合）。
- 偏差来源：未登录态可能遗漏部分弹幕；搜索排序会随时间波动；部分快速 412 导致少量视频弹幕缺失（已留空但保留元数据）。
- 统计结果（Top8 弹幕，次数）：`666`(1126)、`美`(21)、`讲得不错`(20)、`已三连，求资料`(18)、`6666`(17)、`是的打卡学习`(17)、`听说过`(16)、`111`(14)。
- 关键词分布：`大语言模型` 3957，`LLM` 272，`大模型` 198。说明相关视频的弹幕主要集中在“大语言模型”这一表述。

### 3.5 数据可视化界面的展示
- 生成命令：`PYTHONPATH=src MPLCONFIGDIR=./data/processed XDG_CACHE_HOME=./data/processed/.cache python -m danmaku_analysis.cli visualize --font-path <中文字体>`。
- 输出：`data/reports/danmaku_wordcloud.png`。未传字体时会使用默认字体，建议传入中文字体（如苹方/思源）避免乱码。
- 词云分词使用 `jieba`，停用词已过滤常见口水词（哈哈哈、真的、就是等），可按需扩展。
- Excel 报表：`data/reports/danmaku_stats.xlsx`，包含原始弹幕、Top 弹幕、关键词分布三个工作表。

### 3.6 附加题（如有）
- 趋势观点：弹幕高频词集中在“生产力”“三连求资料”“教程”等，表明用户关注学习资料和效率提升；负面或风险相关词较少，说明目前态度偏乐观。
- 可拓展方向：增加科技媒体 RSS 抓取（未实现，可在 `crawler.py` 旁新增模块），或基于用户 ID 哈希做活跃度统计。

## 三、心得体会

- 风控是 B 站抓取的最大不确定性。无 Cookie 场景下，通过降低并发、增加延迟和伪造 buvid 可以拿到一批数据，但仍有少量 412；若需更全数据必须带登录 Cookie。
- 弹幕 XML 的字段偶尔出现非数值（pool/weight），加容错比严格报错更实用。
- Pipeline 命令让“抓取→统计→可视化”一键跑完，减少手工步骤。后续可加重试及失败列表补抓，使数据更完整。